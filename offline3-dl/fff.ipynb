{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rand = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "\n",
    "    def __call__(self,x,train=False):\n",
    "        pass \n",
    "\n",
    "    def backward(self,out_grad,learning_rate):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self,fan_in,fan_out,seed=42):\n",
    "        self.fan_in = fan_in \n",
    "        self.fan_out = fan_out\n",
    "\n",
    "        limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "        self.weights = np.random.uniform(-limit, limit, size=(fan_out, fan_in))\n",
    "        # self.weights = np.random.randn(fan_out,fan_in)\n",
    "        self.bias = np.ones((fan_out,1))\n",
    "        # print(self.weights.shape)\n",
    "        # print(self.grad.shape)\n",
    "    def __call__(self,x,train=False):\n",
    "        self.input = x.reshape(self.fan_in,-1)\n",
    "        return np.dot(self.weights,self.input) + self.bias\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        pass \n",
    "\n",
    "    def backward(self, out_grad, learning_rate ):\n",
    "        wgrad = np.dot(out_grad, self.input.T) \n",
    "        bgrad = out_grad \n",
    "        inputgrad = np.dot(self.weights.T, out_grad)\n",
    "\n",
    "        self.weights -= learning_rate * wgrad \n",
    "        self.bias -= learning_rate * bgrad \n",
    "\n",
    "        # print(f\"{self.wgrad}\")\n",
    "        # print(f\"{self.bgrad}\")\n",
    "\n",
    "        return inputgrad \n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.wgrad = np.zeros((self.fan_in,self.fan_out))\n",
    "        self.bgrad = np.zeros((1,self.fan_out)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __call__(self, input, train=False):\n",
    "        self.input = input\n",
    "        tmp = input - max(input)  \n",
    "        tmp = np.exp(tmp)\n",
    "        self.output = tmp / np.sum(tmp)\n",
    "        return self.output \n",
    "    def backward(self, out_grad, learning_rate):\n",
    "        n = np.size(self.output) \n",
    "        return np.dot((np.identity(n)-self.output.T) * self.output, out_grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self,activation,activation_prime):\n",
    "        self.activation = activation \n",
    "        self.activation_grad = activation_prime\n",
    "\n",
    "    def __call__(self, input, train=False):\n",
    "        self.input = input \n",
    "        return self.activation(self.input) \n",
    "    \n",
    "    def backward(self, out_grad, learning_rate):\n",
    "        return np.multiply(out_grad, self.activation_grad(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "        def tanh_grad(x):\n",
    "            return 1-np.tanh(x)**2 \n",
    "        super().__init__(tanh,tanh_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "        \n",
    "        def sigmoid_grad(x):\n",
    "            return sigmoid(x) * (1.0 - sigmoid(x)) \n",
    "        super().__init__(sigmoid,sigmoid_grad) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        def relu(x):\n",
    "            return np.maximum(0,x)\n",
    "        def relu_grad(x):\n",
    "            return np.where(x > 0, 1, np.where(x < 0, 0, 0.5))\n",
    "        super().__init__(relu,relu_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def __call__(self, x, train=False):\n",
    "        if train:\n",
    "            self.mask = (np.random.rand(*x.shape) < (1 - self.dropout_rate)) / (1 - self.dropout_rate)\n",
    "            # print(self.mask)\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, grad, learning_rate):\n",
    "        return grad * self.mask if self.mask is not None else grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_grad(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_grad(y_true, y_pred): # wrt y_pred\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)\n",
    "\n",
    "def cross_entropy(y_true,y_pred):\n",
    "    return np.mean(-y_true*np.log(y_pred)) \n",
    "\n",
    "def cross_entropy_grad(y_true,y_pred):\n",
    "    return np.mean(-y_true/y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, *layers):\n",
    "        self.layers : Layer = [] \n",
    "        for layer in layers:\n",
    "            self.layers += [layer]\n",
    "        \n",
    "    def __call__(self,input,train=False):\n",
    "        for layer in self.layers:\n",
    "            input = layer(input,train=train)\n",
    "        return input \n",
    "    def backward(self, ):\n",
    "        pass \n",
    "    \n",
    "    def save(self,filename):\n",
    "        pass \n",
    "    def load(self,filename):\n",
    "        pass\n",
    "    \n",
    "    def train_minibatch(self,):\n",
    "        pass \n",
    "    \n",
    "    def train_batch(self, loss, loss_grad, X, y, epochs = 1000, learning_rate = 0.001, verbose=True):\n",
    "        for epoch in range(epochs):\n",
    "            error = 0 \n",
    "            for x, y_ in zip(X,y):\n",
    "                output = self.__call__(x,train=True)\n",
    "                \n",
    "                y__ = y_.reshape(-1,1)\n",
    "                error += loss(y__,output)\n",
    "\n",
    "                grad = loss_grad(y__,output)\n",
    "\n",
    "                for layer in reversed(self.layers):\n",
    "                    grad = layer.backward(grad, learning_rate)\n",
    "            error /= len(X)\n",
    "            if verbose:\n",
    "                print(f\"{epoch=}, {error=}\")\n",
    "\n",
    "    def eval(self, loss, X,y):\n",
    "        error = 0 \n",
    "        for x,y in zip(X,y):\n",
    "            output = self.__call__(x)\n",
    "            error += loss(y,output)\n",
    "        error /= len(X)\n",
    "        return error \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.77395605]\n",
      " [0.43887844]\n",
      " [0.85859792]\n",
      " [0.69736803]\n",
      " [0.09417735]\n",
      " [0.97562235]\n",
      " [0.7611397 ]\n",
      " [0.78606431]\n",
      " [0.12811363]\n",
      " [0.45038594]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.20106312],\n",
       "       [0.2037229 ],\n",
       "       [0.19253629],\n",
       "       [0.18725386],\n",
       "       [0.21542383]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = Linear(10,5)\n",
    "relu = ReLU() \n",
    "sigmoid = Sigmoid()\n",
    "softmax = Softmax()\n",
    "\n",
    "model = NN(lin, sigmoid, relu, softmax) \n",
    "\n",
    "x = rand.random(10).reshape(-1,1)\n",
    "\n",
    "print(x)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.32619805  1.89564367 -0.46529648 -0.24996919  0.35606244 -0.28221255\n",
      "  -0.63704127 -1.04219965 -0.31773012 -0.82844622  0.81787209  0.55289724\n",
      "   0.1380805   1.84531836 -1.14168622 -1.53733126  0.41455222 -1.24364498\n",
      "   0.1708396   0.77796014 -1.22170123 -0.2083445  -0.68965957  3.19704106\n",
      "  -0.23796433  0.08749685  0.33976824  0.4568267  -1.85720938  0.35361752\n",
      "   0.88832478  1.29159404]]\n",
      "[[0.00677718]\n",
      " [0.01277286]\n",
      " [0.22919295]\n",
      " [0.01829279]\n",
      " [0.03795207]\n",
      " [0.00547625]\n",
      " [0.01020633]\n",
      " [0.60284563]\n",
      " [0.05204314]\n",
      " [0.0244408 ]] 1.0\n"
     ]
    }
   ],
   "source": [
    "model = NN(Linear(32,5), ReLU(), Linear(5,10), Softmax()) \n",
    "\n",
    "x = np.random.randn(1,32)\n",
    "\n",
    "print(x)\n",
    "result = model(x) \n",
    "print(result, np.sum(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms \n",
    "\n",
    "train_validation_dataset = datasets.EMNIST(root='./data', \n",
    "                                           split='letters',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True,\n",
    "                                        )\n",
    "\n",
    "\n",
    "independent_test_dataset = datasets.EMNIST(\n",
    "                             root='./data',\n",
    "                             split='letters',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor(),\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_validation_dataset)\n",
    "n_classes = len(train_validation_dataset.classes)\n",
    "n_datapoints = len(train_validation_dataset.targets)\n",
    "\n",
    "X_train = np.array(train_validation_dataset.data.reshape(-1,28*28))\n",
    "# y_train = np.zeros((len(train_validation_dataset.targets),len(train_validation_dataset.classes)))\n",
    "\n",
    "y_train = np.array([[1 if temp == i else 0 for i in range(n_classes)] for temp in train_validation_dataset.targets])\n",
    "y_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(independent_test_dataset.data.reshape(-1,28*28))\n",
    "y_test = np.array([[1 if temp == i else 0 for i in range(n_classes)] for temp in independent_test_dataset.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(train_validation_dataset.data.reshape(-1,28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   778  779  780  781  782  783  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(Linear(28, 26), Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(Linear(28*28,1024), ReLU(), Dropout(0.2), Linear(1024,n_classes), Softmax(), Dropout(0.3), Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, error=0.007693150207259264\n",
      "epoch=1, error=0.007740308105596984\n",
      "epoch=2, error=0.0077259086732750825\n"
     ]
    }
   ],
   "source": [
    "model.train_batch(cross_entropy, cross_entropy_grad, X_train, y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_test[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
