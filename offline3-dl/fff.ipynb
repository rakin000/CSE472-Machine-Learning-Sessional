{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rand = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "\n",
    "    def __call__(self,x,train=False):\n",
    "        pass \n",
    "\n",
    "    def backward(self,out_grad,learning_rate):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self,fan_in,fan_out,seed=42):\n",
    "        self.fan_in = fan_in \n",
    "        self.fan_out = fan_out\n",
    "\n",
    "        #xavier init \n",
    "        limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "        self.weights = np.random.uniform(-limit, limit, size=(fan_out, fan_in))\n",
    "        self.bias = np.ones((fan_out,1))\n",
    "\n",
    "    def __call__(self,x,train=False):\n",
    "        self.input = x\n",
    "        return np.dot(self.weights,self.input) + self.bias\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        pass \n",
    "\n",
    "    def backward(self, out_grad, learning_rate ):\n",
    "        wgrad = np.dot(out_grad, self.input.T) / np.size(out_grad, axis=1) # mean  \n",
    "        bgrad = np.mean(out_grad, axis=1, keepdims=True) \n",
    "        inputgrad = np.dot(self.weights.T, out_grad)\n",
    "\n",
    "        self.weights -= learning_rate * wgrad \n",
    "        self.bias -= learning_rate * bgrad \n",
    "\n",
    "        # print(f\"{self.wgrad}\")\n",
    "        # print(f\"{self.bgrad}\")\n",
    "\n",
    "        return inputgrad \n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.wgrad = np.zeros((self.fan_in,self.fan_out))\n",
    "        self.bgrad = np.zeros((1,self.fan_out)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __call__(self, input, train=False):\n",
    "        self.input = input\n",
    "        tmp = input - np.max(input, axis=0)  \n",
    "        tmp = np.exp(tmp)\n",
    "        self.output = tmp / np.sum(tmp, axis=0, keepdims=True)\n",
    "        return self.output \n",
    "    def backward(self, out_grad, learning_rate):\n",
    "        n = np.size(self.output, axis=0) \n",
    "        grad = np.hstack([ np.dot( (np.identity(n) - input.T )*input, out_grad) for input in self.input.T  ])\n",
    "        return grad #np.dot((np.identity(n)-self.output.T) * self.output, out_grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self,activation,activation_prime):\n",
    "        self.activation = activation \n",
    "        self.activation_grad = activation_prime\n",
    "\n",
    "    def __call__(self, input, train=False):\n",
    "        self.input = input \n",
    "        return self.activation(self.input) \n",
    "    \n",
    "    def backward(self, out_grad, learning_rate):\n",
    "        return np.multiply(out_grad, self.activation_grad(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "        def tanh_grad(x):\n",
    "            return 1-np.tanh(x)**2 \n",
    "        super().__init__(tanh,tanh_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "        \n",
    "        def sigmoid_grad(x):\n",
    "            return sigmoid(x) * (1.0 - sigmoid(x)) \n",
    "        super().__init__(sigmoid,sigmoid_grad) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        def relu(x):\n",
    "            return np.maximum(0,x)\n",
    "        def relu_grad(x):\n",
    "            return np.where(x > 0, 1, np.where(x < 0, 0, 0.5))\n",
    "        super().__init__(relu,relu_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def __call__(self, x, train=False):\n",
    "        if train:\n",
    "            self.mask = (np.random.rand(*x.shape) < (1 - self.dropout_rate)) / (1 - self.dropout_rate)\n",
    "            # print(self.mask)\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, grad, learning_rate):\n",
    "        return grad * self.mask if self.mask is not None else grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_grad(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_grad(y_true, y_pred): # wrt y_pred\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)\n",
    "\n",
    "def cross_entropy(y_true,y_pred,epsilon=1e-15):\n",
    "    y_pred = np.clip( y_pred, epsilon, 1.0-epsilon)\n",
    "    return np.mean(-y_true*np.log(y_pred)) \n",
    "\n",
    "def cross_entropy_grad(y_true,y_pred,epsilon=1e-15):\n",
    "    y_pred = np.clip( y_pred, epsilon, 1.0-epsilon)\n",
    "    return (-y_true/y_pred) / len(y_true)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, *layers):\n",
    "        self.layers : Layer = [] \n",
    "        for layer in layers:\n",
    "            self.layers += [layer]\n",
    "        \n",
    "    def __call__(self,input,train=False):\n",
    "        for layer in self.layers:\n",
    "            input = layer(input,train=train)\n",
    "        return input \n",
    "    def backward(self, ):\n",
    "        pass \n",
    "    \n",
    "    def save(self,filename):\n",
    "        pass \n",
    "    def load(self,filename):\n",
    "        pass\n",
    "    \n",
    "    def train_minibatch(self,):\n",
    "        pass \n",
    "    \n",
    "    def train(self, loss, loss_grad, X, y, epochs = 1000, batch_size = 8, learning_rate = 0.001, verbose=True):\n",
    "        shuffled_indices = np.arange(len(X))\n",
    "        np.random.shuffle(shuffled_indices) \n",
    "\n",
    "        X = X[shuffled_indices]\n",
    "        y = y[shuffled_indices] \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            error = 0 \n",
    "\n",
    "            for i in range((len(X)+batch_size-1)//batch_size):\n",
    "                X_batch = X[i:i+batch_size].T\n",
    "                y_batch = y[i:i+batch_size].T\n",
    "\n",
    "                y_pred = self.__call__(X_batch,train=True)\n",
    "                error += loss(y_batch,y_pred)\n",
    "\n",
    "                grad = loss_grad(y_batch,y_pred)\n",
    "\n",
    "                for layer in reversed(self.layers):\n",
    "                    grad = layer.backward(grad, learning_rate)\n",
    "            error /= len(X)\n",
    "            if verbose:\n",
    "                print(f\"{epoch=}, {error=}\")\n",
    "\n",
    "    def eval(self, X,y):\n",
    "        corr = 0 \n",
    "        for x,y in zip(X,y):\n",
    "            y_pred = self.__call__(x)\n",
    "            corr += 1 if np.argmax(y_pred) == np.argmax(y) else 0 \n",
    "        \n",
    "        return corr/len(X) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms \n",
    "\n",
    "train_validation_dataset = datasets.EMNIST(root='./data', \n",
    "                                           split='letters',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True,\n",
    "                                        )\n",
    "\n",
    "\n",
    "independent_test_dataset = datasets.EMNIST(\n",
    "                             root='./data',\n",
    "                             split='letters',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor(),\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_validation_dataset)\n",
    "n_classes = len(train_validation_dataset.classes)\n",
    "n_datapoints = len(train_validation_dataset.targets)\n",
    "\n",
    "X_train = np.array(train_validation_dataset.data.reshape(-1,28*28))\n",
    "# y_train = np.zeros((len(train_validation_dataset.targets),len(train_validation_dataset.classes)))\n",
    "\n",
    "y_train = np.array([[1 if temp == i else 0 for i in range(n_classes)] for temp in train_validation_dataset.targets])\n",
    "y_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(independent_test_dataset.data.reshape(-1,28*28))\n",
    "y_test = np.array([[1 if temp == i else 0 for i in range(n_classes)] for temp in independent_test_dataset.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(train_validation_dataset.data.reshape(-1,28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(Linear(28*28,1024), ReLU(), Dropout(0.2), Linear(1024,n_classes), ReLU(), Dropout(0.3), Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, error=1.5341182674297338e-05\n",
      "epoch=1, error=1.3935674039037677e-05\n",
      "epoch=2, error=1.2982227036972222e-05\n",
      "epoch=3, error=1.2300383413063474e-05\n",
      "epoch=4, error=1.1698609802370086e-05\n",
      "epoch=5, error=1.107696364799779e-05\n",
      "epoch=6, error=1.062510406754492e-05\n",
      "epoch=7, error=1.0207003265852734e-05\n",
      "epoch=8, error=9.885472741422722e-06\n",
      "epoch=9, error=9.658039603395563e-06\n"
     ]
    }
   ],
   "source": [
    "model.train(cross_entropy, cross_entropy_grad, X_train, y_train, batch_size=1000, epochs=10, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train[:4].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:4].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
